{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-31T21:26:12.640803Z",
     "start_time": "2025-05-31T21:26:12.638590Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder"
   ],
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T21:26:12.803094Z",
     "start_time": "2025-05-31T21:26:12.801250Z"
    }
   },
   "cell_type": "code",
   "source": "## Functions implementation:",
   "id": "9a8631653d3a4b01",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T21:26:12.955991Z",
     "start_time": "2025-05-31T21:26:12.944825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "###Function used to eliminate the categorical features that have been eliminated for the specified reasons.\n",
    "# Returns only the categorical features that will be used.###\n",
    "def clean_categorical_data(data):\n",
    "    data = data.copy()\n",
    "\n",
    "    # Binary variable with 8 value of 'y' and 500k+ of 'n'\n",
    "    data.drop(['pymnt_plan', 'application_type'], axis=1, inplace=True)\n",
    "\n",
    "    # Dropping due to high amount of NaN values in column\n",
    "    data.drop([\n",
    "        'mths_since_last_delinq', 'mths_since_last_record',\n",
    "        'mths_since_last_major_derog', 'verification_status_joint', 'desc'\n",
    "    ], axis=1, inplace=True)\n",
    "\n",
    "    # Dropping because of high cardinality\n",
    "    data.drop(['emp_title', 'title', 'batch_enrolled'], axis=1, inplace=True)\n",
    "\n",
    "    # Dropping because no pattern was found between state and loan status\n",
    "    data.drop(['zip_code', 'addr_state', 'sub_grade'], axis=1, inplace=True)\n",
    "\n",
    "    # Categorical features cleanup\n",
    "    data.loc[:, 'term'] = data.loc[:, 'term'].apply(lambda x: int(x.split(' ')[0]))\n",
    "    data.loc[:, 'initial_list_status'] = data.loc[:, 'initial_list_status'].apply(lambda x: 0 if x == 'f' else 1)\n",
    "\n",
    "    data.loc[:, 'emp_length'] = data.loc[:, 'emp_length'].fillna('Missing')\n",
    "    data.loc[:, 'emp_length'] = data.loc[:, 'emp_length'].str.replace('(year.*)', '', regex=True)\n",
    "    data.loc[:, 'emp_length'] = data.loc[:, 'emp_length'].str.replace(' ', '')\n",
    "    data.loc[:, 'emp_length'] = data.loc[:, 'emp_length'].str.replace('<1', '0')\n",
    "\n",
    "    categorical_variables = ['term', 'emp_length', 'initial_list_status']\n",
    "    return data.loc[:, categorical_variables]\n"
   ],
   "id": "82bd320758568558",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T21:26:13.100150Z",
     "start_time": "2025-05-31T21:26:13.097479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Label encodes the categorical features and generates a dictionary with a LabelEncoder for each feature to be used to prepare the testing training_data.\n",
    "def labelencode_categorical_data(data):\n",
    "\n",
    "  data = data.copy()\n",
    "\n",
    "  categorical_variables = data.columns\n",
    "\n",
    "  categorical_label_encoders = {x:LabelEncoder() for x in categorical_variables}\n",
    "\n",
    "  for variable in categorical_variables:\n",
    "    categorical_label_encoders[variable].fit(data[variable])\n",
    "    data[variable] = categorical_label_encoders[variable].transform(data[variable])\n",
    "\n",
    "  return data ,categorical_label_encoders"
   ],
   "id": "415febfce4f7c9f0",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T21:26:13.269786Z",
     "start_time": "2025-05-31T21:26:13.265458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_numerical_data(data):\n",
    "  data = data.copy()\n",
    "\n",
    "  numerical_variables = ['last_week_pay', 'loan_amnt', 'funded_amnt', 'dti', 'int_rate', 'annual_inc', 'delinq_2yrs', 'inq_last_6mths', 'open_acc', 'total_rec_int', 'pub_rec', 'last_week_pay', 'revol_util', 'acc_now_delinq', 'tot_coll_amt', 'tot_cur_bal', 'total_rev_hi_lim']\n",
    "\n",
    "  #Determines how many weeks since last payment for the loan.\n",
    "  def last_week_pay_parse(x):\n",
    "    x = x.replace('th week', '')\n",
    "    if x == 'NA':\n",
    "      return 0\n",
    "    else:\n",
    "      return int(x)\n",
    "\n",
    "  data.loc[:, 'last_week_pay'] = data.loc[:, 'last_week_pay'].apply(lambda x: last_week_pay_parse(x))\n",
    "\n",
    "  #It was found that most of the numerical features had a consistent number of outliers and also, null values.\n",
    "  #To fill these values, the mean of the values between the non-outliers values was calculated.\n",
    "  for variable in numerical_variables:\n",
    "    q1 = data[variable].quantile(0.25)\n",
    "    q3 = data[variable].quantile(0.75)\n",
    "\n",
    "    if q1 == 0 and q3 == 0:\n",
    "      data[variable].fillna(0.0, inplace=True)\n",
    "    else:\n",
    "\n",
    "      iqr = q3 - q1\n",
    "      limit1 = q1 - 1.5 * iqr\n",
    "      limit2 = q3 + 1.5 * iqr\n",
    "\n",
    "      #mean is extracted for that variable and then, used to fill the nan.\n",
    "      mean = np.nanmean(\n",
    "        np.extract(\n",
    "          (data[variable] >= limit1) &\n",
    "          (data[variable] <= limit2),\n",
    "          arr = data[variable]\n",
    "        )\n",
    "      )\n",
    "      data[variable].fillna(mean, inplace=True)\n",
    "\n",
    "  return data[numerical_variables]"
   ],
   "id": "5a345da39f2b2821",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T21:26:13.428815Z",
     "start_time": "2025-05-31T21:26:13.424738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "###It was observed that it was possible to reduce the number of features by combining them to obtain ratios that will also add insights. ###\n",
    "def feature_engineering(data):\n",
    "    data = data.copy()\n",
    "    # Create ratio columns with proper handling of zero denominators\n",
    "    data.loc[:, 'inc_loan_ratio'] = (data.loc[:, 'annual_inc'] / data.loc[:, 'loan_amnt'].apply(lambda x: 1 if x == 0 else x)).astype(np.float16)\n",
    "    data.loc[:, 'borrow_loan_ratio'] = (data.loc[:, 'total_rev_hi_lim'] / data.loc[:, 'loan_amnt'].apply(lambda x: 1 if x == 0 else x)).astype(np.float16)\n",
    "    # Create binary delinquency indicator\n",
    "    data.loc[:, 'is_delinquent'] = data.loc[:, 'acc_now_delinq'].apply(lambda x: 0 if x == 0 else 1).astype(np.int8)\n",
    "    # Calculate funded ratio\n",
    "    data.loc[:, 'funded_ratio'] = (data.loc[:, 'loan_amnt'] / data.loc[:, 'funded_amnt']).astype(np.float16)\n",
    "    # Drop redundant or now-unnecessary columns\n",
    "    data.drop(['annual_inc', 'loan_amnt', 'total_rev_hi_lim', 'acc_now_delinq'], axis=1, inplace=True)\n",
    "    return data"
   ],
   "id": "99bc940060fa7a32",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T21:26:13.586360Z",
     "start_time": "2025-05-31T21:26:13.583959Z"
    }
   },
   "cell_type": "code",
   "source": [
    "###The numerical features were scaling between 0 and 1 to improve the model's efficiency of prediction.\n",
    "# This returns the numerical variables scaled and the scaler that will be later used for the testing training_data.###\n",
    "def min_max_scaling(data):\n",
    "  min_max_scaler = MinMaxScaler()\n",
    "  min_max_scaler.fit(data)\n",
    "  scaled_data = min_max_scaler.transform(data)\n",
    "  return pd.DataFrame(scaled_data, columns = data.columns), min_max_scaler"
   ],
   "id": "cdbecce6db1853f8",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T21:26:13.739466Z",
     "start_time": "2025-05-31T21:26:13.737547Z"
    }
   },
   "cell_type": "code",
   "source": "## Data Loading:",
   "id": "6a03770215bd08ea",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T21:26:15.084261Z",
     "start_time": "2025-05-31T21:26:13.889262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_data = pd.read_csv('../raw_data/loan_requests_data.csv')\n",
    "train_data.drop('member_id', axis = 1, inplace = True)"
   ],
   "id": "1e47676537811f51",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T21:26:15.109311Z",
     "start_time": "2025-05-31T21:26:15.107910Z"
    }
   },
   "cell_type": "code",
   "source": "## Data cleaning:",
   "id": "a44e3d83044210f8",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T21:26:16.307722Z",
     "start_time": "2025-05-31T21:26:15.181749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Categorical features are processed and cleaned:\n",
    "clean_categorical_data = clean_categorical_data(train_data)\n",
    "encoded_categorical_data, label_encoders = labelencode_categorical_data(clean_categorical_data)\n",
    "resulting_categorical_columns = list(encoded_categorical_data.columns)"
   ],
   "id": "8d3370b9d5bdf256",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T21:26:18.633543Z",
     "start_time": "2025-05-31T21:26:16.326664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Numerical features are processed and cleaned:\n",
    "clean_numerical_data = clean_numerical_data(train_data)\n",
    "featured_engineered_data = feature_engineering(clean_numerical_data)\n",
    "scaled_numerical_data, min_max_scaler = min_max_scaling(featured_engineered_data)"
   ],
   "id": "3fe61b58b05de5a1",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T21:26:18.682245Z",
     "start_time": "2025-05-31T21:26:18.662936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Data is finally combined :\n",
    "data = pd.concat([encoded_categorical_data, scaled_numerical_data], axis=1)\n",
    "data.columns = resulting_categorical_columns + list(scaled_numerical_data.columns)\n",
    "\n",
    "data['loan_status'] = train_data['loan_status'].astype(np.int8)"
   ],
   "id": "5e98b618bf775661",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T21:26:21.071517Z",
     "start_time": "2025-05-31T21:26:18.721126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#It was discovered that the dataset had a huge imbalance. Given that there are plenty of rows for each case,\n",
    "#rows with the most populated loan status value were randomly eliminated until there were exactly the same of number for both categories.\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "y = data['loan_status']\n",
    "data.drop('loan_status', axis = 1, inplace = True)\n",
    "X, y = rus.fit_resample(data, y)\n",
    "\n",
    "data = X\n",
    "data['loan_status'] = y\n",
    "\n",
    "#Data is stored to train the model in train_model.py\n",
    "data.to_csv('../training_data/balanced_training_data.csv', index = False)"
   ],
   "id": "4d1ab1d395be246e",
   "outputs": [],
   "execution_count": 72
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

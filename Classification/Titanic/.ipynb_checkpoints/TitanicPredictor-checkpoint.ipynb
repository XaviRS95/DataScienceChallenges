{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77145536-c7e6-4c83-a564-243a037b7199",
   "metadata": {},
   "source": [
    "# Titanic Death/Survivor Predictor Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3ede8f-b9e8-4bd1-88cc-e59e3bc225cb",
   "metadata": {},
   "source": [
    "Hi there! This notebook consists on a step-by step of the mental process I followed during this challenge . It includes different techniques I studied and applied to not only improve the accuracy of the used models, but also to learn about how these techniques work and how to use them.\n",
    "\n",
    "The original challenge alongside the data and what each feature represents can be found in https://www.kaggle.com/competitions/titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fa3d82-76e2-40d5-8eb3-0dcb974d2dce",
   "metadata": {},
   "source": [
    "## Data Loading and libraries importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92b52a87-4f18-467b-8158-2ac32e90a579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57ac954e-dd5e-4417-a34c-b32b7b6d6ffd",
   "metadata": {},
   "outputs": [],
   "source": "train_data = pd.read_csv('training_data/train.csv')"
  },
  {
   "cell_type": "markdown",
   "id": "175b4382-1bca-44af-9c91-3d706b1617d2",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee24108-16f7-46b6-9615-077d84cf1a5c",
   "metadata": {},
   "source": [
    "From a first look at the dataset, there are 11 features and 1 binary target variable with 891 rows in total. From these 11 features, only Age, Embarked and Cabin include missing values.\n",
    "Additionally, some features can be redundant and|or not generate any insights after examination. It's also worth mentioning that 61% of the passengers died while approximately only 39% survived, which indicates an imbalance in the dataset that might have to be dealt with while prepraing the data for the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "756c57a7-6221-4e9a-92b4-13caf2860f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId 0\n",
      "Survived 0\n",
      "Pclass 0\n",
      "Name 0\n",
      "Sex 0\n",
      "Age 177\n",
      "SibSp 0\n",
      "Parch 0\n",
      "Ticket 0\n",
      "Fare 0\n",
      "Cabin 687\n",
      "Embarked 2\n"
     ]
    }
   ],
   "source": [
    "for column in train_data.columns:\n",
    "    print(column, train_data[column].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0064cd65-2a48-4d43-8600-d25258c6deca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      int64\n",
       "Survived         int64\n",
       "Pclass           int64\n",
       "Name            object\n",
       "Sex             object\n",
       "Age            float64\n",
       "SibSp            int64\n",
       "Parch            int64\n",
       "Ticket          object\n",
       "Fare           float64\n",
       "Cabin           object\n",
       "Embarked        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f6974eb-a5b3-4ac2-bac1-6a73859ab276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 12)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e40c489-fc4f-4a50-8359-15a4fc120e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived\n",
       "0    549\n",
       "1    342\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['Survived'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a9c1182-2f12-464a-85d4-a813942260f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ticket\n",
       "347082      7\n",
       "CA. 2343    7\n",
       "1601        7\n",
       "3101295     6\n",
       "CA 2144     6\n",
       "           ..\n",
       "9234        1\n",
       "19988       1\n",
       "2693        1\n",
       "PC 17612    1\n",
       "370376      1\n",
       "Name: count, Length: 681, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['Ticket'].value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6a2b8f98-d61c-4aad-9ad3-7b283610c71c",
   "metadata": {},
   "source": [
    "Studying the passenger's name, id, the ticket and the cabin features bring these conclusions:\n",
    "-PassengerId is a simple Id for every passenger, so it will be eliminated.\n",
    "-Name could be used to extract the sex of the passenger, but we already have that info.\n",
    "-Cabin includes multiple cabins in what seems like that person booked multiple rooms next to each other (Family and relatives perhaps?).\n",
    "-Ticket numbers can include a number, a prefix + numbers and a whole number. According to https://www.encyclopedia-titanica.org/community/threads/ticket-numbering-system.20348/page-2, the XX code could be the place where the ticket was bought but apart from that there isn't much information to make out of it. For now it will also be discarded.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cafc3d1-b9ab-4289-b7ef-d8cb195bcaed",
   "metadata": {},
   "source": [
    "## Data cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb5bc44-ad78-4bfe-bf6a-3825267e90a8",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2feb659-3f13-4729-9697-a7c5f49f3cff",
   "metadata": {},
   "source": [
    "First, the previously mentioned features will be eliminated and the Sex variable will be changed to 0 and 1 values. After that, the Embarked feature had 2 missing values, but first the non-va values were encoded to 0, 1 and 2 to finally change the nans by its median:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c869c48-35bd-4f4b-b64b-3f0785d21c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop(['PassengerId','Name','Ticket','Cabin'], axis=1, inplace=True)\n",
    "#In this case when applying there's no nan values, otherwise it will all change them all to 1.\n",
    "train_data['Sex'] = train_data['Sex'].apply(lambda x: 0 if x=='male' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "509f58d3-a37e-4324-8bb0-92a0755f6690",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since Embarked has 2 missing values, it has to be dealt with first:\n",
    "train_data.loc[train_data['Embarked'].notna(), 'Embarked'] = train_data.loc[train_data['Embarked'].notna(), 'Embarked'].apply(lambda x: 0 if x == 'S' else 1 if x == 'C' else 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93e83eed-8263-4321-a1f2-e915ba6ba994",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Embarked'].fillna(train_data['Embarked'].median(),  inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da73a259-abd3-49db-bb00-570bc36987f1",
   "metadata": {},
   "source": [
    "#### Dealing with the Age variable problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a6cb2a-bbfd-40a1-8f15-d8d2a09f9a21",
   "metadata": {},
   "source": [
    "Age has a total of 177 missing values. There are 2 different approaches I want to try and study their RMSE value to establish which one is the most accurate:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2557991-8cfa-4aa0-9f32-e5e5de7db915",
   "metadata": {},
   "source": [
    "##### Filling the NA using a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e021eea3-1f98-46c7-9306-5f77a498125f",
   "metadata": {},
   "source": [
    "Linear Regression and Random Forest Regressor were used to calculate the Age missing values. ###TO-DO: INCLUDE A CORRELATION STUDY FOR WHAT FEATURES WILL BE USED BY THE MODEL TO CALCULATE THE AGE. In this case, the data wasn't scaled because RMSE didn't change.\n",
    "\n",
    "The RMSE indicates that the model with the least error is LinearRegression. (###Still need to include hyper-parameter tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57fa22f-3767-4985-9f1a-ffd0a2d347a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "###TO-DO: INCLUDE A CORRELATION STUDY FOR WHAT FEATURES WILL BE USED BY THE MODEL TO CALCULATE THE AGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "66371790-9f19-4739-bcb6-f53cb72bf1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data[train_data['Age'].notna()][['Sex', 'Pclass', 'Fare' ,'Age']]\n",
    "y = X['Age'].astype(np.int8)\n",
    "X = X.drop('Age', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "506b751e-72f7-4708-aaed-b2677a6eb11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression_age = LinearRegression().fit(X_train, y_train)\n",
    "random_forest_age = RandomForestRegressor().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "316cd54d-6978-4811-85da-f35095fdfc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_predictions = linear_regression_age.predict(X_test)\n",
    "RF_predictions = random_forest_age.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "44b4a070-ecb8-4bae-92ff-6f614f4d616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_lr = np.sqrt(mean_squared_error(y_test, LR_predictions))\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, RF_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "74405e55-649a-4fd8-b224-68d146c972d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12.682856570364436, 14.010247183953606)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_lr, rmse_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4cbe92-df23-4d92-b70b-5b40942749aa",
   "metadata": {},
   "source": [
    "##### Filling the NA using mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8f390f-d42e-49c6-924e-64457b3d62fe",
   "metadata": {},
   "source": [
    "A numpy array of the same length as y_test with all values as the mean of X_train['Age'] to measure its RMSE. It has a value higher than the Linear Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1eb42fe-59b5-4898-abe2-6f444c16e060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.708608790726196"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_predicted = y_train.mean()\n",
    "np.sqrt(mean_squared_error(y_test, np.full((y_test.shape[0],1), mean_predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b8a1a3-62d7-4581-b0ba-7c83e084ab8a",
   "metadata": {},
   "source": [
    "##### Using the LR predicted values to fill the missing ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2850079f-1e00-46dc-84f1-7f90cbb38d63",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45160/1529663355.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  age_na_train['Age'] = linear_regression_age.predict(X).astype(int)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1db6a8f-6a7a-4f9d-8bad-e97aa4f02058",
   "metadata": {},
   "source": [
    "### Addressing the imbalance of Survivors/Deceased:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ceda4f1-1eea-4f6a-8461-d6298cd2a74b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "y = clean_train['Survived']\n",
    "X = clean_train.drop('Survived', axis=1)\n",
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "# fit and apply the transform\n",
    "X_over, y_over = oversample.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc682729-659b-4256-abd0-447faf8b28a0",
   "metadata": {},
   "source": [
    "### Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "985ce8d7-bb8b-470a-8b2e-a43cfa350dff",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_over)\n",
    "MinMaxScaler()\n",
    "scaled_features = scaler.transform(X_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e69ca3-f046-48dc-bcb1-466d6046c55c",
   "metadata": {},
   "source": [
    "## Training the binary classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d88a239c-434d-450a-9a29-0a91d573cff2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(scaled_features, y_over, test_size=0.2, random_state=42)\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bdbfdc7b-0b31-4c68-9a63-facfd99ab5a6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.61904761904762 %\n",
      "Recall: 72.11538461538461 %\n",
      "Precision: 80.64516129032258 %\n",
      "F1 Score: 76.14213197969544 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(max_iter = 1000)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "print('Accuracy:',accuracy_score(y_test, y_pred)*100,'%')\n",
    "print('Recall:', recall_score(y_test, y_pred)*100,'%')\n",
    "print('Precision:', precision_score(y_test, y_pred)*100,'%')\n",
    "print('F1 Score:', f1_score(y_test, y_pred)*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3423f3f4-0076-45b6-b809-8cb1f220d32d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.19047619047619 %\n",
      "Recall: 88.46153846153845 %\n",
      "Precision: 84.40366972477065 %\n",
      "F1 Score: 86.3849765258216 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "print('Accuracy:',accuracy_score(y_test, y_pred)*100,'%')\n",
    "print('Recall:', recall_score(y_test, y_pred)*100,'%')\n",
    "print('Precision:', precision_score(y_test, y_pred)*100,'%')\n",
    "print('F1 Score:', f1_score(y_test, y_pred)*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "79b64272-c270-470f-91cb-0f7c637b93b0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.19047619047619 %\n",
      "Recall: 88.46153846153845 %\n",
      "Precision: 84.40366972477065 %\n",
      "F1 Score: 86.3849765258216 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=2) \n",
    "knn.fit(X_train, y_train)\n",
    "y_predict = knn.predict(X_test)\n",
    "print('Accuracy:',accuracy_score(y_test, y_pred)*100,'%')\n",
    "print('Recall:', recall_score(y_test, y_pred)*100,'%')\n",
    "print('Precision:', precision_score(y_test, y_pred)*100,'%')\n",
    "print('F1 Score:', f1_score(y_test, y_pred)*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aec277a-f237-4e9d-86f4-e6ca31ccf6a4",
   "metadata": {},
   "source": [
    "## Preparing the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "557ad2ba-6133-414b-bdbc-1de1cb7bc0a8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45160/796778557.py:6: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  test_data['Embarked'].fillna('2', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv('Titanic/test.csv')\n",
    "columns_to_eliminate = ['Name','Ticket','Cabin']\n",
    "test_data.drop(columns_to_eliminate, axis=1, inplace=True)\n",
    "test_data['Sex'] = test_data['Sex'].apply(lambda x: 0 if x=='male' else 1)\n",
    "test_data['Embarked'] = train_data['Embarked'].apply(lambda x: 0 if x == 'S' else 1 if x == 'C' else 2)\n",
    "test_data['Embarked'].fillna('2', inplace=True)\n",
    "test_data['Fare'].fillna(test_data['Fare'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f76b0fa9-a0e2-45b3-bbc1-9080fd4055f7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "age_na_test = test_data[test_data['Age'].isna()]\n",
    "age_na_test_passenger_id = age_na_test['PassengerId']\n",
    "age_na_test = age_na_test[['Sex', 'Pclass', 'Fare' ,'Age']]\n",
    "full_age_test = test_data.dropna()\n",
    "\n",
    "\n",
    "X = age_na_test.drop('Age', axis=1)\n",
    "age_na_test['Age'] = linear_regression_age.predict(X).astype(int)\n",
    "age_na_test['PassengerId'] = age_na_test_passenger_id\n",
    "prepared_test = pd.concat([age_na_test, full_age_test])\n",
    "prepared_test['Age'] = prepared_test['Age'].astype(int)\n",
    "\n",
    "\n",
    "prepared_test = prepared_test.set_index('PassengerId').join(\n",
    "    test_data[['PassengerId', 'SibSp', 'Parch', 'Embarked']].set_index('PassengerId'), \n",
    "    lsuffix='_l', \n",
    "    rsuffix='', \n",
    "    on='PassengerId')\n",
    "\n",
    "\n",
    "prepared_test.drop(['SibSp_l', 'Parch_l', 'Embarked_l'], axis = 1, inplace=True)\n",
    "\n",
    "\n",
    "scaled_test = scaler.transform(prepared_test[clean_train.columns[1:]])\n",
    "\n",
    "\n",
    "final_dataset = pd.DataFrame()\n",
    "final_dataset['PassengerId'] = prepared_test.index\n",
    "final_dataset['Survived'] = rf.predict(scaled_test)\n",
    "\n",
    "\n",
    "final_dataset.sort_values(by=['PassengerId'], inplace=True)\n",
    "\n",
    "\n",
    "final_dataset.to_csv('Titanic/results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c4d55d-27e4-45d9-b4a8-790122bb745d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3658a661-2a4c-49f2-a33f-290e629c254c",
   "metadata": {},
   "source": [
    "## Loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4be4a6eb-16dd-4cfc-8653-95f50fc4706c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04d2684c-90f1-4cbf-9284-978861880e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cc97b1-9c87-443a-8b31-20dc44e14abc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Understanding the data first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bb374e-0ce9-4d3b-a143-bf5d47a51232",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77be4a72-1e08-4b71-b892-f72cc7fa54d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774e87af-870c-4b4e-914e-859da43d82b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f6108f-18dd-45df-bbca-1d5cd4631aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f97138-c2e9-4ade-ad05-af57add278ce",
   "metadata": {},
   "source": [
    "## Data Preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "621f7e9e-462d-4f41-8151-42676f1633ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Transported'] = train_data['Transported'].apply(lambda x: 1 if x else 0)\n",
    "\n",
    "train_data['Total_Spent'] = train_data[['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].sum(axis=1, numeric_only=True)\n",
    "train_data.drop(['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck'], axis = 1, inplace = True)\n",
    "\n",
    "train_data['Total_Spent_Band'] = pd.cut(train_data['Total_Spent'], 7)\n",
    "train_data.drop(['Total_Spent'], axis = 1, inplace = True)\n",
    "\n",
    "label_encoder_total_spent_band = LabelEncoder()\n",
    "label_encoder_total_spent_band.fit(train_data['Total_Spent_Band'])\n",
    "nan_index = train_data['Total_Spent_Band'].isna()\n",
    "train_data['Total_Spent_Band'] = label_encoder_total_spent_band.transform(train_data['Total_Spent_Band'])\n",
    "train_data.loc[nan_index,'Total_Spent_Band'] = int(train_data['Total_Spent_Band'].median()) \n",
    "\n",
    "train_data['Group'] = train_data['PassengerId'].apply(lambda x: int(x.split('_')[1]))\n",
    "\n",
    "train_data['Age'].fillna(int(train_data['Age'].mean(skipna = True)), inplace = True)\n",
    "train_data['Age_Band'] = pd.cut(train_data['Age'], 7)\n",
    "train_data.drop(['Age'], axis = 1, inplace = True)\n",
    "\n",
    "label_encoder_age_band = LabelEncoder()\n",
    "label_encoder_age_band.fit(train_data['Age_Band'])\n",
    "nan_index = train_data['Age_Band'].isna()\n",
    "train_data['Age_Band'] = label_encoder_age_band.transform(train_data['Age_Band'])\n",
    "train_data.loc[nan_index,'Age_Band'] = int(train_data['Age_Band'].median())  \n",
    "\n",
    "train_data[['Deck', 'Num', 'Side']] = train_data['Cabin'].str.split('/', expand = True)\n",
    "train_data.drop('Cabin', axis = 1, inplace = True)\n",
    "\n",
    "label_encoder_deck = LabelEncoder()\n",
    "label_encoder_deck.fit(train_data['Deck'])\n",
    "nan_index = train_data['Deck'].isna()\n",
    "train_data['Deck'] = label_encoder_deck.transform(train_data['Deck'])\n",
    "train_data.loc[nan_index,'Deck'] = int(train_data['Deck'].median())   \n",
    "\n",
    "label_encoder_side = LabelEncoder()\n",
    "label_encoder_side.fit(train_data['Side'])\n",
    "nan_index = train_data['Side'].isna()\n",
    "train_data['Side'] = label_encoder_side.transform(train_data['Side'])\n",
    "train_data.loc[nan_index,'Side'] = int(train_data['Side'].median()) \n",
    "\n",
    "label_encoder_home_planet = LabelEncoder()\n",
    "label_encoder_home_planet.fit(train_data['HomePlanet'])\n",
    "nan_index = train_data['HomePlanet'].isna()\n",
    "train_data['HomePlanet'] = label_encoder_home_planet.transform(train_data['HomePlanet'])\n",
    "train_data.loc[nan_index,'HomePlanet'] = int(train_data['HomePlanet'].median())\n",
    "\n",
    "label_encoder_vip = LabelEncoder()\n",
    "label_encoder_vip.fit(train_data['VIP'])\n",
    "nan_index = train_data['VIP'].isna()\n",
    "train_data['VIP'] = label_encoder_vip.transform(train_data['VIP'])\n",
    "train_data.loc[nan_index,'VIP'] = int(train_data['VIP'].median())  \n",
    "\n",
    "label_encoder_cryo_sleep = LabelEncoder()\n",
    "label_encoder_cryo_sleep.fit(train_data['CryoSleep'])\n",
    "nan_index = train_data['CryoSleep'].isna()\n",
    "train_data['CryoSleep'] = label_encoder_cryo_sleep.transform(train_data['CryoSleep'])\n",
    "train_data.loc[nan_index,'CryoSleep'] = int(train_data['CryoSleep'].median())  \n",
    "\n",
    "label_encoder_cryo_destination = LabelEncoder()\n",
    "label_encoder_cryo_destination.fit(train_data['Destination'])\n",
    "nan_index = train_data['Destination'].isna()\n",
    "train_data['Destination'] = label_encoder_cryo_destination.transform(train_data['Destination'])\n",
    "train_data.loc[nan_index,'Destination'] = int(train_data['Destination'].median())\n",
    "\n",
    "train_data[['First_Name', 'Last_Name']] = train_data['Name'].str.split(' ', expand=True)\n",
    "\n",
    "train_data.to_csv('clean_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33889000-2965-4460-a523-b6600d3c47f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataprocessing(data):\n",
    "\n",
    "    data.drop('Name', axis = 1, inplace = True)\n",
    "    \n",
    "    data['Total_Spent'] = data[['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].sum(axis=1, numeric_only=True)\n",
    "    data.drop(['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck'], axis = 1, inplace = True)\n",
    "    data['Total_Spent_Band'] = pd.cut(data['Total_Spent'], 7)\n",
    "    data.drop(['Total_Spent'], axis = 1, inplace = True)\n",
    "\n",
    "    data['Age'].fillna(int(data['Age'].mean(skipna = True)), inplace = True)\n",
    "    data['Age_Band'] = pd.cut(data['Age'], 7)\n",
    "    data.drop(['Age'], axis = 1, inplace = True)\n",
    "    \n",
    "    data['Group'] = data['PassengerId'].apply(lambda x: int(x.split('_')[1]))\n",
    "    data.drop('PassengerId', axis = 1, inplace = True)\n",
    "    \n",
    "    data[['Deck', 'Num', 'Side']] = data['Cabin'].str.split('/', expand = True)\n",
    "    data.drop('Num', axis = 1, inplace = True)\n",
    "    data.drop('Cabin', axis = 1, inplace = True)\n",
    "    data['Deck'].fillna('Missing', inplace = True)\n",
    "    data['Side'].fillna('Missing', inplace = True)\n",
    "\n",
    "    data['Destination'].fillna('Missing', inplace = True)\n",
    "    data['CryoSleep'] = train_data['CryoSleep'].apply(lambda x: 1 if x else 0)\n",
    "    data['VIP'] = train_data['CryoSleep'].apply(lambda x: 1 if x else 0)\n",
    "\n",
    "    data = pd.get_dummies(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d33c4e3-1edd-4518-a03f-31e30ce0062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "target_train = train_data['PassengerId']\n",
    "clean_train_data = dataprocessing(train_data)\n",
    "clean_train_data['Transported'] = clean_train_data['Transported'].apply(lambda x: 1 if x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebfc0ac0-e9e3-4be4-8352-72aa462c22e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(clean_train_data, target_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4113e4be-698b-4cca-b78e-69e712294643",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "print('Accuracy:',accuracy_score(y_test, y_pred)*100,'%')\n",
    "print('Recall:', recall_score(y_test, y_pred)*100,'%')\n",
    "print('Precision:', precision_score(y_test, y_pred)*100,'%')\n",
    "print('F1 Score:', f1_score(y_test, y_pred)*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6797be96-da7d-4e87-afa0-c8c8b47cbf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(max_iter = 1000)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "print('Accuracy:',accuracy_score(y_test, y_pred)*100,'%')\n",
    "print('Recall:', recall_score(y_test, y_pred)*100,'%')\n",
    "print('Precision:', precision_score(y_test, y_pred)*100,'%')\n",
    "print('F1 Score:', f1_score(y_test, y_pred)*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ecfff7-ca72-4c76-a955-556019b46c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=2) \n",
    "knn.fit(X_train, y_train)\n",
    "y_predict = knn.predict(X_test)\n",
    "print('Accuracy:',accuracy_score(y_test, y_pred)*100,'%')\n",
    "print('Recall:', recall_score(y_test, y_pred)*100,'%')\n",
    "print('Precision:', precision_score(y_test, y_pred)*100,'%')\n",
    "print('F1 Score:', f1_score(y_test, y_pred)*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135997ea-4cae-4c32-ae2d-29cc42efc376",
   "metadata": {},
   "source": [
    "## First Assumptions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6122438-6742-4d86-ad7b-75fdd0fb4575",
   "metadata": {},
   "source": [
    "### Understand how each feature is correlated with the Transported variable:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9506f637-c6cb-4202-a06c-408069c5b5b1",
   "metadata": {},
   "source": [
    "In this case, it doesn't seem to be any variable that right away might be correlated, so an exploratory analysis will be necessary:\n",
    "\n",
    "For the first step, let's start studying the groups they were in, the home planet, their cryosleep status, their location in the ship (based on the cabin deck, number and side), their destination and age ranges and VIP status."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fba474-3b3c-42b5-bd5d-5dbf5edb0bab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Group variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1db6bc-9bb2-4c7c-80e1-44ac62af61c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transported = [0,1]\n",
    "for value in train_data['Group'].unique():\n",
    "    for i in transported:\n",
    "        print('Group', value, 'with transported:',i,'. Had a total of:',len(train_data.query('Group == '+str(value)+' & Transported == '+str(i))))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42038b9d-7833-4417-9c43-b7b89a71f1b8",
   "metadata": {},
   "source": [
    "This indicates that there is no correlation between the groups and the number of transportations, so it's discarded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fbb855-951f-4931-ad9c-a0b1ae6ec78d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Cabin variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94390633-e81b-44b1-a974-63d41a8c485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transported = [0,1]\n",
    "for value in train_data['Deck'].unique():\n",
    "    for i in transported:\n",
    "        print('Deck', value, 'with transported:',i,'. Had a total of:',len(train_data.query('Deck == '+str(value)+' & Transported == '+str(i))))\n",
    "    print('-------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560caff6-0655-49a2-83f9-7e81fbbbbd82",
   "metadata": {},
   "source": [
    "The most populated decks were 1, 2 and 3. The decks 1, 4 and 2 had a vast difference when it came to transported passengers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd37ef45-b398-4e09-8a30-6c164b85921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transported = [0,1]\n",
    "for value in train_data['Side'].unique():\n",
    "    for i in transported:\n",
    "        print('Side', value, 'with transported:',i,'. Had a total of:',len(train_data.query('Side == '+str(value)+' & Transported == '+str(i))))\n",
    "    print('-------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25118a0-7b82-4e90-9cc2-1ff0712ae272",
   "metadata": {},
   "source": [
    "The side variable also doesn't show any indications of a different distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baf2d72-a19b-4096-8f2d-26036818703d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Money spent aboard variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfed095b-57aa-467a-b1ac-819cb5f5a645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba890691-bf04-4070-85b4-78808bdd6566",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Age variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1273d7-fde7-4ef1-8276-0b5dc73b1419",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Age_Band'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5274c71d-f83b-4267-8fda-41125ffe69b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.query('Transported == 1')['Age_Band'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018e4707-e948-444e-a00c-528b0c892ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.query('Transported == 0')['Age_Band'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc1bf5f-063f-4266-8b38-e2bfa2138908",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### VIP variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9f2ffd-10b2-49e4-8907-f862a5822dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.query('Transported == 0')['VIP'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37727512-c8c3-4f9b-904f-d4925488007c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.query('Transported == 1')['VIP'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d71d40-f7e5-4ca3-b1ec-e0ebbe4b05c5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### HomePlanet and Destination variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe79b97-1a4c-490c-99fe-4fc727e8f8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.query('Transported == 0')['HomePlanet'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642da1f1-7a40-4228-a9de-62794935d8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.query('Transported == 1')['HomePlanet'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be35752-2b96-4cbb-a4fd-1762222d7d6f",
   "metadata": {},
   "source": [
    "Europa travellers had more proportional cases in comparison to other home planets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3cdf44-524d-4d54-b150-a76198af161d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### CryoSleep variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e95cf731-f64f-459c-aeb4-11ed114429a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CryoSleep\n",
       "0    3761\n",
       "1     554\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.query('Transported == 0')['CryoSleep'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "947bbe1d-2856-4e4f-b223-01a11ccd9c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CryoSleep\n",
       "1    2483\n",
       "0    1895\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.query('Transported == 1')['CryoSleep'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5dcbb2-bc98-4494-b4a5-53de1b722057",
   "metadata": {},
   "source": [
    "For now it's safe to say that those who were not in cryosleep were not transported"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024d1dfc-76d5-4928-8089-a11a94e14ca9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Destination variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e083b4c4-4c78-4fdb-9bfb-cac70c168f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.query('Transported == 0')['Destination'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43eb7b2-f399-408e-9d6d-991bcde9c18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.query('Transported == 1')['Destination'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf9f876-d96e-48be-94b6-737b60074fd6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b0d4a6-8eb1-40f1-b738-81ca10360fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_female = open('female.txt','r')\n",
    "female_names = file_female.read().splitlines()\n",
    "file_female.close()\n",
    "file_male = open('male.txt','r')\n",
    "male_names = file_male.read().splitlines()\n",
    "file_male.close()\n",
    "\n",
    "female_dataset = pd.DataFrame({'Name':female_names, 'Gender':0})\n",
    "male_dataset = pd.DataFrame({'Name':male_names, 'Gender':1})\n",
    "names_dataset1 = pd.concat([male_dataset, female_dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660445f8-ac29-4679-96d6-aa61a8964d96",
   "metadata": {},
   "source": [
    "Names Corpus, Version 1.3 (1994-03-29)\n",
    "Copyright (C) 1991 Mark Kantrowitz\n",
    "Additions by Bill Ross\n",
    "\n",
    "This corpus contains 5001 female names and 2943 male names, sorted\n",
    "alphabetically, one per line.\n",
    "\n",
    "You may use the lists of names for any purpose, so long as credit is\n",
    "given in any published work. You may also redistribute the list if you\n",
    "provide the recipients with a copy of this README file. The lists are\n",
    "not in the public domain (I retain the copyright on the lists) but are\n",
    "freely redistributable.  If you have any additions to the lists of\n",
    "names, I would appreciate receiving them.\n",
    "\n",
    "Mark Kantrowitz <mkant+@cs.cmu.edu>\n",
    "http://www-2.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c86034-c2cd-4a67-aee9-822c564d59b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://archive.ics.uci.edu/dataset/591/gender+by+name\n",
    "names_dataset2 = pd.read_csv('name_gender_dataset.csv')\n",
    "names_dataset2 = names_dataset2[['Name','Gender']]\n",
    "names_dataset2['Gender'] = names_dataset2['Gender'].apply(lambda x: 0 if x == 'F' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a27ab0c-a49d-4fef-82ce-cf596721072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_dataset = pd.concat([names_dataset1, names_dataset2]).drop_duplicates()\n",
    "names_list = names_dataset['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21be1fe1-9654-4124-ad1b-a2744fc45a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_names_dataset_indexes = train_data['First_Name'].isna()\n",
    "non_nan_names_dataset = train_data['First_Name'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a7816b-a868-41cf-aaa8-53d1c394069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_name = \"Aamir\"\n",
    "names_dataset.query('Name == \"'+other_name+'\"')['Gender'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6775e3c9-0b75-4660-b865-8f8baedbd00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignGender(name):\n",
    "    aux_similar_gender = (0,0)\n",
    "    print(name)\n",
    "    for other_name in names_list:\n",
    "        if name.lower() in other_name.lower():\n",
    "            if name.lower() == other_name.lower():\n",
    "                gender = names_dataset.query('Name == \"'+other_name+'\"')['Gender'].values[0]\n",
    "                print(name, gender)\n",
    "                return gender\n",
    "            else:\n",
    "                score = len(name)/len(other_name)\n",
    "                if aux_similar_gender[1] < score:\n",
    "                    aux_similar_gender = (names_dataset.query('Name == \"'+other_name+'\"')['Gender'].values[0], score)\n",
    "        else:\n",
    "            aux_counter = 0\n",
    "            i = 0\n",
    "            while i < len(name) and i < len(other_name):\n",
    "                if name.lower()[i] == other_name.lower()[i]:\n",
    "                    aux_counter += 1\n",
    "                i += 1\n",
    "            score = aux_counter/len(other_name)\n",
    "            if score > aux_similar_gender[1]:\n",
    "                aux_similar_gender = (names_dataset.query('Name == \"'+other_name+'\"')['Gender'].values[0], score)\n",
    "                      \n",
    "    return aux_similar_gender[0]\n",
    "\n",
    "\n",
    "non_nan_first_names_index = train_data['First_Name'].notna()\n",
    "non_nan_first_names_data = train_data[train_data['First_Name'].notna()]\n",
    "\n",
    "train_data.loc[non_nan_first_names_index,'Sex'] = non_nan_first_names_data['First_Name'].apply(lambda x: assignGender(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dee9809-8d0e-4492-a14e-b6ed0796e430",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Sex'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a049a99b-5f23-4378-83dd-910d7b62ce0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.query('T')['Sex'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
